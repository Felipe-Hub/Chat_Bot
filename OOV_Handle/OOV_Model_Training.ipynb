{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package brown to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potions: re, numpy as np, pandas as pd, pickle, json, nltk, keras, collections\n",
      "spells: clean_data, predict_tags, tagged_n_grams, unknown_words_X, check_and_predict\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.python.keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout, Input, Concatenate\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../utils')\n",
    "from NLP_little_helpers import *\n",
    "little_helpers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences/Tags data length: 289401 289401\n",
      "('<BOS> they do not <EOS>', '<BOS> they do to <EOS>', '<BOS> i hope so <EOS>')\n",
      "('<start> PPSS DO * <end>', '<start> PPSS DO TO <end>', '<start> NN NN RB <end>')\n"
     ]
    }
   ],
   "source": [
    "# Movie conversation cleaned and POS tagged data\n",
    "with open('clean_tagged_data.json', 'r') as outfile:\n",
    "    data = json.load(outfile)\n",
    "    \n",
    "sent, tags = zip(*data['tags'])\n",
    "\n",
    "print('Sentences/Tags data length:', len(sent), len(tags))\n",
    "print(sent[:3])\n",
    "print(tags[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent[:20000]\n",
    "tags = tags[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of n-grams to build sequences\n",
    "n = 5\n",
    "\n",
    "# maximum length of sequence is n-1 since the last word will be the target (y)\n",
    "max_length = n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 779764\n",
      "Vocabulary Size: 12133\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and getting n-gram sequences\n",
    "\n",
    "tk_text = Tokenizer(filters=[])\n",
    "tk_text.fit_on_texts(sent)\n",
    "enc_sentences = tk_text.texts_to_sequences(sent)\n",
    "\n",
    "tk_tags = Tokenizer(filters=[])\n",
    "tk_tags.fit_on_texts(tags)\n",
    "enc_tagged = tk_tags.texts_to_sequences(tags)\n",
    "\n",
    "tagged_sent = zip(enc_sentences, enc_tagged)\n",
    "\n",
    "text_grams, tag_grams = tagged_n_grams(tagged_sent, n)\n",
    "\n",
    "X, y, X_rev, y_rev = text_grams\n",
    "X_tag, y_tag, X_tag_rev, y_tag_rev = tag_grams\n",
    "\n",
    "vocab_size = len(tk_text.word_index)+1\n",
    "tags_size = len(tk_tags.word_index)+1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'they', 'do', 'not', '<eos>'],\n",
       " ['<bos>', 'they', 'do', 'to', '<eos>']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting sentences for word2vec\n",
    "splt_sent = [s.lower().split() for s in sent]\n",
    "splt_sent[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding words with word2vec (skip-gram)\n",
    "embedded = Word2Vec(splt_sent, sg=1, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embedding_matrix to associate embedding with encoded sequences\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tk_text.word_index.items():\n",
    "    embedding_matrix[i] = embedded.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tk_text, f)\n",
    "    \n",
    "with open('tags_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tk_tags, f)\n",
    "    \n",
    "with open('max_length.pkl', 'wb') as f:\n",
    "    pickle.dump(max_length, f)\n",
    "    \n",
    "with open('w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(embedded, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Training for OOV Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using n-grams of sentences, tags, reversed sentences and reversed tags\n",
    "# using word2vec embedding for sentences and standard keras embedding for tags\n",
    "# concatenating each model to get a more reliable prediction \n",
    "\n",
    "text_input = Input((max_length,))\n",
    "model_text = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(text_input)\n",
    "model_text = Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.1))(model_text)\n",
    "\n",
    "tags_input = Input((max_length,))\n",
    "model_tags = Embedding(vocab_size, 100)(tags_input)\n",
    "model_tags = Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.1))(model_tags)\n",
    "\n",
    "concatenate_text_tags = Concatenate()([model_text, model_tags])\n",
    "\n",
    "rev_text_input = Input((max_length,))\n",
    "rev_model_text = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(rev_text_input)\n",
    "rev_model_text = Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.1))(rev_model_text)\n",
    "\n",
    "rev_tags_input = Input((max_length,))\n",
    "rev_model_tags = Embedding(vocab_size, 100)(rev_tags_input)\n",
    "rev_model_tags = Bidirectional(LSTM(16, dropout=0.2, recurrent_dropout=0.1))(rev_model_tags)\n",
    "\n",
    "rev_concatenate_text_tags = Concatenate()([rev_model_text, rev_model_tags])\n",
    "\n",
    "concatenate = Concatenate()([concatenate_text_tags, rev_concatenate_text_tags])\n",
    "\n",
    "result = Dense(vocab_size+1, activation='softmax')(concatenate) # check this\n",
    "\n",
    "model = Model(inputs=[text_input, tags_input, rev_text_input, rev_tags_input], outputs=result)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(0.001),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "model.fit([X, X_tag, X_rev, X_tag_rev], y, batch_size=128, epochs=5, verbose=1, shuffle=True, validation_split=0.2)\n",
    "\n",
    "model.save('model_full_concat.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
