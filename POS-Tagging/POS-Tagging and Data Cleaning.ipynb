{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package brown to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potions: re, numpy as np, pandas as pd, pickle, json, nltk, keras, collections\n",
      "spells: clean_data, predict_tags, tagged_n_grams, unknown_words_X, check_and_predict\n"
     ]
    }
   ],
   "source": [
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../utils')\n",
    "from NLP_little_helpers import *\n",
    "little_helpers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training HMM for POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate words and tags in corpus\n",
    "\n",
    "words = [i[0] for i in [list(zip(*c)) for c in corpus]]\n",
    "tags = [i[1] for i in [list(zip(*c)) for c in corpus]]\n",
    "\n",
    "assert len(words) == len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / Testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(words, tags, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary used for training\n",
    "\n",
    "train_words = list(set([word for words in X_train for word in words]))\n",
    "train_tags = list(set([word for words in y_train for word in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions calls\n",
    "\n",
    "emission_counts = pt.pair_counts(X_train, y_train)\n",
    "tag_unigrams = pt.unigram_counts(y_train)\n",
    "tag_bigrams = pt.bigram_counts(y_train)\n",
    "tag_starts = pt.starting_counts(y_train)\n",
    "tag_ends = pt.ending_counts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "model = HiddenMarkovModel(name=\"brown-hmm-tagger\")\n",
    "\n",
    "\n",
    "# Create states with emission probability distributions P(word | tag) and add to the model\n",
    "\n",
    "states = {}\n",
    "\n",
    "for tags, words in emission_counts.items():\n",
    "    n = tag_unigrams[tags]\n",
    "    prob = {word:count/n for word, count in words.items()}\n",
    "    emissions = DiscreteDistribution(prob)\n",
    "    state = State(emissions, name=tags)\n",
    "    states[tags] = state\n",
    "    model.add_states(state)\n",
    "   \n",
    "\n",
    "    \n",
    "# Add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n",
    "\n",
    "for tags, counts in tag_starts.items():\n",
    "    model.add_transition(model.start, states[tags], counts/sum(tag_starts.values()))\n",
    "\n",
    "for (tag1, tag2), counts in tag_bigrams.items():\n",
    "    model.add_transition(states[tag1], states[tag2], counts/tag_unigrams[tag1])\n",
    "\n",
    "for tags, counts in tag_ends.items():\n",
    "    model.add_transition(states[tags], model.end, counts/tag_unigrams[tags])\n",
    "    \n",
    "\n",
    "# Laplace smoothing:\n",
    "\n",
    "tag_bigrams_test = pt.bigram_counts(y_test)\n",
    "denominator = len(train_tags)\n",
    "\n",
    "for (tag1, tag2), counts in tag_bigrams_test.items():\n",
    "    if (tag1, tag2) in tag_bigrams:\n",
    "        continue\n",
    "    if tag1 not in states or tag2 not in states:\n",
    "        continue\n",
    "    if tag1 in tag_unigrams:\n",
    "        denominator += tag_unigrams[tag1]\n",
    "    model.add_transition(states[tag1], states[tag2], 1/denominator)\n",
    "\n",
    "    \n",
    "model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('hmm_model.pkl', 'wb') as outfile:\n",
    "    pickle.dump(model, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data and Predict POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Trained_Weights/hmm_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 144001: expected 5 fields, saw 6\\nSkipping line 144113: expected 5 fields, saw 6\\nSkipping line 144283: expected 5 fields, saw 6\\nSkipping line 144328: expected 5 fields, saw 6\\nSkipping line 144337: expected 5 fields, saw 6\\nSkipping line 144400: expected 5 fields, saw 6\\nSkipping line 144438: expected 5 fields, saw 6\\nSkipping line 225183: expected 5 fields, saw 9\\nSkipping line 225288: expected 5 fields, saw 41\\nSkipping line 225302: expected 5 fields, saw 6\\nSkipping line 225394: expected 5 fields, saw 6\\nSkipping line 225625: expected 5 fields, saw 6\\n'\n"
     ]
    }
   ],
   "source": [
    "to_tag = pd.read_csv('movie_lines.txt', sep='|', error_bad_lines=False,\n",
    "                                   encoding= 'unicode_escape', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movie_conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ea15556bd58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_conversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'movie_conversation' is not defined"
     ]
    }
   ],
   "source": [
    "data = clean_data(to_tag[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbis_dict = predict_tags(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_tagged_data.json', 'w') as outfile:\n",
    "    json.dumps(viterbis_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
