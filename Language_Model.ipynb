{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7989803587745975418\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5651511826185801252\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10967262691985611617\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2435186688\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6877443409011915869\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from tensorflow.python import keras\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "config = tf.compat.v1.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "#from keras import backend\n",
    "#backend.set_session(session)\n",
    "#print(backend._get_available_gpus())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potions: re, numpy as np, pandas as pd, pickle, json, nltk, keras, collections\n",
      "spells: clean_data, clean_entire, predict_tags, tagged_3D, tagged_n_grams, unknown_words_X, check_and_predict\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.models import Sequential, Input, Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.python.keras.layers import Dot, Average, Dense, Dropout, RepeatVector, Conv2D, Flatten, MaxPooling2D, TimeDistributed, BatchNormalization, LSTM, Concatenate, Embedding, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from random import randint, choice\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../utils')\n",
    "from NLP_little_helpers import *\n",
    "little_helpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(X, X_tag, n=4):\n",
    "    \"\"\"\n",
    "    This function takes a sequence of tupled sentences and tags:\n",
    "    [('Here is an example', 'tag1 tag2 tag3 tag4')]\n",
    "    Returns a tuple with other two tuples inside and the target (y).\n",
    "    The first tuple is for the encoded texts and the second for the encoded tags.\n",
    "    Each tuple contains two arrays:\n",
    "    1. The first array contains padded n-grams combinations(X).\n",
    "    2. The second array is composed of the next n words.\n",
    "    \n",
    "    All returned sequences are already pre-padded with 0.\n",
    "    The maximum length of the sequences is n, with n being the desired n-grams.\n",
    "    \"\"\"  \n",
    "    \n",
    "    text_sequences = list()\n",
    "    tags_sequences = list()\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        text_sent_sequences = list()\n",
    "        tags_sent_sequences = list()\n",
    "        first_sent_words = list()\n",
    "        first_sent_tags = list()\n",
    "        \n",
    "        for j in range(len(X[i])):\n",
    "            \n",
    "            first_words = X[i][:j+2]\n",
    "            first_tags = X_tag[i][:j+2]\n",
    "            text_sequence = X[i][j:j+n+1]\n",
    "            tags_sequence = X_tag[i][j:j+n+1]\n",
    "            # avoinding repeated n-grams in the same sentence\n",
    "            if text_sequence not in text_sent_sequences and len(text_sequence)>1 and len(tags_sequence)>1 and len(text_sequence)<=n:\n",
    "                text_sent_sequences += [text_sequence]\n",
    "                tags_sent_sequences += [tags_sequence]\n",
    "                first_sent_words += [first_words]\n",
    "                first_sent_tags += [first_tags]\n",
    "\n",
    "        if len(text_sent_sequences)>1:\n",
    "            text_sequences += first_sent_words + text_sent_sequences\n",
    "            tags_sequences += first_sent_tags + tags_sent_sequences\n",
    "\n",
    "    print('Total Sequences: %d' % len(text_sequences))\n",
    "    X = pad_sequences([seq[:-1] for seq in text_sequences], maxlen=n)\n",
    "    X_tag = pad_sequences([seq[:-1] for seq in tags_sequences], maxlen=n)\n",
    "    y = np.array([[seq[-1]] for seq in text_sequences])\n",
    "    assert len(X) == len(y) == len(X_tag)\n",
    "    \n",
    "    return X, X_tag, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 4158864\n",
      "<bos> you know you are in love when you cannot fall asleep because reality is finally better than your dreams <eos> \n",
      " <bos> PPSS VB PPSS BER IN NN WRB PPSS MD* VB RB CS NN BEZ RB RBR CS PP$ NNS <eos>\n",
      "Vocabulary Size: 183296\n",
      "Unique Tags: 42\n",
      "Max length: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/quotes_tagged.json', 'r') as outfile:\n",
    "    viterbis_dict = json.load(outfile)\n",
    "      \n",
    "text = [' '.join([['<bos>'] + words + ['<eos>']][0]) for words, tags in viterbis_dict if tags != 'UNK']\n",
    "tags = [' '.join([['<bos>'] + tags + ['<eos>']][0]) for words, tags in viterbis_dict if tags != 'UNK']\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(filters='', lower=False)\n",
    "tokenizer.fit_on_texts(text+tags)\n",
    "enc_texts = tokenizer.texts_to_sequences(text)\n",
    "enc_tags = tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "seq2word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "with open('../Trained_Weights/tokenizer_dict.json', 'w') as file:\n",
    "    json.dump(seq2word_map, file)\n",
    "    \n",
    "#with open('../Trained_Weights/tokenizer_text.pkl', 'wb') as file:\n",
    "#    pickle.dump(t_text, file)\n",
    "    \n",
    "#with open('../Trained_Weights/tokenizer_tags.pkl', 'wb') as file:\n",
    "#    pickle.dump(t_tags, file)\n",
    "\n",
    "#max_length = max([len(sent) for sent in enc_texts])\n",
    "tags_size = len(set([t for tag in tags for t in tag]))\n",
    "vocab_size = len(set([word for words in enc_texts for word in words]))\n",
    "embed_size = tags_size+vocab_size\n",
    "\n",
    "X_text, X_tags, y = n_grams(enc_texts, enc_tags, n=5)\n",
    "\n",
    "max_length = X_text.shape[1]\n",
    "\n",
    "print(text[0], '\\n', tags[0])\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "print('Unique Tags: %d' % tags_size)\n",
    "print('Max length:', max_length)\n",
    "\n",
    "y_weight = [j for i in y for j in i]\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_weight), y_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove embedding\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(\"../Data/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183534"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[76]\n",
    "X_text[10]\n",
    "tokenizer.word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text1 = [list(x) for x in X_text]\n",
    "X_tags1 = [list(x) for x in X_tags]\n",
    "X = list(zip(X_text1, X_tags1))\n",
    "X = [list(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "text = [[['<bos>'] + words + ['<eos>']][0] for words, tags in viterbis_dict if tags != 'UNK']\n",
    "\n",
    "tags = [[['<bos>'] + tags + ['<eos>']][0] for words, tags in viterbis_dict if tags != 'UNK']\n",
    "\n",
    "# embedding words with word2vec (skip-gram)\n",
    "embedded = Word2Vec(tags+text, sg=1, min_count=0, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_w2v = np.zeros((len(tokenizer.word_index.items())+1, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_matrix_w2v[i] = embedded.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = []\n",
    "for text, tags in X:\n",
    "    for i in range(len(text)):\n",
    "        text[i] = embedding_matrix[text[i]]\n",
    "        tags[i] = embedding_matrix_w2v[tags[i]]\n",
    "        X_new += [np.multiply(text[i], tags[i]/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2, 5, 300)]       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 1, 3000)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 16)                192640    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 183297)            3116049   \n",
      "=================================================================\n",
      "Total params: 3,308,689\n",
      "Trainable params: 3,308,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.python.keras.Input((2,5,300))\n",
    "model_text = tf.python.keras.layers.Flatten()(inputs)\n",
    "model_text = tf.python.keras.layers.RepeatVector(1)(model_text)\n",
    "model_text = tf.python.keras.layers.Bidirectional(tf.python.keras.layers.CuDNNLSTM(8))(model_text)\n",
    "result = tf.python.keras.layers.Dense(vocab_size+1, activation='softmax')(model_text)\n",
    "model_text = tf.python.keras.Model(inputs, result)\n",
    "\n",
    "model_text.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text.fit(X, y,\n",
    "          batch_size=4, epochs=1, verbose=1,\n",
    "          class_weight = class_weights,\n",
    "          shuffle=True, validation_split=0.2,\n",
    "          workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text.save('../Trained_Weights/tagged_quotes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 5, 300)       54989400    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 5, 300)       72000       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 5, 300)       0           embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 5, 4)         4848        average[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 5, 1)         5           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 4)            64          time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 183298)       916490      bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 55,982,807\n",
      "Trainable params: 993,407\n",
      "Non-trainable params: 54,989,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using n-grams of sentences, tags, reversed sentences and reversed tags\n",
    "# concatenating each model to get a more reliable prediction \n",
    "with tf.device('/cpu:0'):  \n",
    "    text_input = Input((max_length,))\n",
    "    model_text = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], trainable=False)(text_input)\n",
    "\n",
    "#model_text = LSTM(8, return_sequences=True, dropout=0.2, activation='sigmoid')(model_text)\n",
    "#model_text = TimeDistributed(Dense(1, activation='selu'))(model_text)\n",
    "#model_text = LSTM(4, dropout=0.2)(model_text)\n",
    "\n",
    "tags_input = Input((max_length,))\n",
    "model_tags = Embedding(tags_size, 300)(tags_input)\n",
    "#model_tags = LSTM(8, return_sequences=True, dropout=0.2, activation='sigmoid')(model_tags)\n",
    "#model_tags = TimeDistributed(Dense(1, activation='selu'))(model_tags)\n",
    "#model_tags = LSTM(4, dropout=0.2)(model_tags)\n",
    "\n",
    "concatenate = Average()([model_text, model_tags])\n",
    "\"\"\"\n",
    "rev_text_input = Input((max_length,))\n",
    "rev_model_text = Embedding(embed_size, 64)(rev_text_input)\n",
    "rev_model_text = Bidirectional(LSTM(8, dropout=0.2, recurrent_dropout=0.1))(rev_model_text)\n",
    "\n",
    "rev_tags_input = Input((max_length,))\n",
    "rev_model_tags = Embedding(embed_size, 64)(rev_tags_input)\n",
    "rev_model_tags = Bidirectional(LSTM(8, dropout=0.2, recurrent_dropout=0.1))(rev_model_tags)\n",
    "\n",
    "rev_concatenate_text_tags = Concatenate()([rev_model_text, rev_model_tags])\n",
    "\n",
    "concatenate = Concatenate()([concatenate_text_tags, rev_concatenate_text_tags])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#model = RepeatVector(1)(concatenate)\n",
    "model = Bidirectional(LSTM(2, dropout=0.2, return_sequences=True, activation='selu'))(concatenate)\n",
    "model = TimeDistributed(Dense(1, activation='selu'))(model)\n",
    "model = Bidirectional(LSTM(2, dropout=0.2, activation='selu'))(model)\n",
    "\n",
    "result = Dense(vocab_size+2, activation='softmax')(model)\n",
    "\n",
    "model = Model(inputs=[text_input, tags_input], outputs=result)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3327091 samples, validate on 831773 samples\n",
      "Epoch 1/2\n",
      "3327091/3327091 [==============================] - 4603s 1ms/sample - loss: 2.8410 - acc: 0.6292 - val_loss: 2.8444 - val_acc: 0.6345\n",
      "Epoch 2/2\n",
      "3327091/3327091 [==============================] - 3772s 1ms/sample - loss: 2.7305 - acc: 0.6340 - val_loss: 2.8338 - val_acc: 0.6351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff19b42c050>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks=[EarlyStopping(patience=1, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath='model_last.{epoch:02d}-{val_loss:.2f}.h5')]\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([X, X_tags], y,\n",
    "          callbacks=callbacks,\n",
    "          batch_size=64, epochs=2, verbose=1,\n",
    "          class_weight = class_weights,\n",
    "          shuffle=True, validation_split=0.2,\n",
    "          workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Trained_Weights/tagged_quotes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/100_books_tagged.json', 'r') as outfile:\n",
    "    viterbis_dict = json.load(outfile)\n",
    "      \n",
    "text = [[['<bos>'] + words + ['<eos>']][0] for words, tags in viterbis_dict if tags != 'UNK']\n",
    "tags = [[['<bos>'] + tags + ['<eos>']][0] for words, tags in viterbis_dict if tags != 'UNK']\n",
    "\n",
    "\n",
    "# integer encode sequences of words\n",
    "# integer encode sequences of words\n",
    "t_text = Tokenizer(filters='', oov_token=1)\n",
    "t_text.fit_on_texts(text)\n",
    "enc_texts = t_text.texts_to_sequences(text)\n",
    "\n",
    "t_tags = Tokenizer(filters='', oov_token=1)\n",
    "t_tags.fit_on_texts(tags)\n",
    "enc_tags = t_tags.texts_to_sequences(tags)\n",
    "\n",
    "seq2word_map = dict(map(reversed, t.word_index.items()))\n",
    "\n",
    "with open('../Trained_Weights/tokenizer_dict.json', 'w') as file:\n",
    "    json.dump(seq2word_map, file)\n",
    "    \n",
    "with open('../Trained_Weights/tokenizer.pkl', 'w') as file:\n",
    "    pickle.dump(t, file)\n",
    "\n",
    "#max_length = max([len(sent) for sent in enc_texts])\n",
    "tags_size = len(set([t for tag in tags for t in tag]))\n",
    "vocab_size = len(set([word for words in enc_texts for word in words]))\n",
    "embed_size = tags_size+vocab_size\n",
    "\n",
    "X, X_tags, y = n_grams(enc_texts, enc_tags, n=20)\n",
    "\n",
    "max_length = X.shape[1]\n",
    "\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "print('Unique Tags: %d' % tags_size)\n",
    "print('Max length:', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2D = np.array(list(zip(X, X_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 2, 5, 64)          5516800   \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 2, 5, 512)         524800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 1, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 1, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 7183)              7362575   \n",
      "_________________________________________________________________\n",
      "repeat_vector_9 (RepeatVecto (None, 1, 7183)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio (None, 1, 256)            7487488   \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 1, 86200)          22153400  \n",
      "=================================================================\n",
      "Total params: 43,047,111\n",
      "Trainable params: 43,046,087\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_text = Sequential()\n",
    "model_text.add(Embedding(vocab_size+1, 64, input_shape=(2,max_length)))\n",
    "\n",
    "model_text.add(Conv2D(512, kernel_size=4, padding='same', activation='relu'))\n",
    "model_text.add(Dropout(0.2))\n",
    "model_text.add(MaxPooling2D(2, 4, padding='same'))\n",
    "model_text.add(BatchNormalization())\n",
    "model_text.add(Flatten())\n",
    "model_text.add(Dense(vocab_size/12, activation='selu'))\n",
    "#model_text.add(Dense(1, activation='selu'))\n",
    "model_text.add(RepeatVector(1))\n",
    "model_text.add(Bidirectional(LSTM(128, dropout = 0.2, return_sequences=True)))\n",
    "\n",
    "\"\"\"\n",
    "model_text.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))\n",
    "model_text.add(Dropout(0.5))\n",
    "model_text.add(Conv2D(4, kernel_size=4, padding='same', activation='relu'))\n",
    "model_text.add(BatchNormalization())\n",
    "model_text.add(Flatten())\n",
    "model_text.add(Dropout(0.5))\n",
    "\"\"\"\n",
    "\n",
    "model_text.add(TimeDistributed(Dense(vocab_size+1, activation='softmax')))\n",
    "\n",
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[EarlyStopping(patience=1, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath='model_last.{epoch:02d}-{val_loss:.2f}.h5')]\n",
    "\n",
    "model_text.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_text.fit(X_2D, y,\n",
    "          callbacks=callbacks,\n",
    "          batch_size=32, epochs=3, verbose=1,\n",
    "          shuffle=True, validation_split=0.2,\n",
    "          workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text.save('conv_100_books.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59336, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 5, 300)            25688400  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 8)                 9760      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 85627)             770643    \n",
      "=================================================================\n",
      "Total params: 26,468,803\n",
      "Trainable params: 780,403\n",
      "Non-trainable params: 25,688,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):      \n",
    "    model_text = Sequential()\n",
    "    model_text.add(Embedding(len(embedding_matrix), 300, input_length=max_length,\n",
    "                             weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model_text.add(Bidirectional(LSTM(4, dropout = 0.2)))\n",
    "#model_text.add(TimeDistributed(Dense(1, activation='selu')))\n",
    "#model_text.add(LSTM(8, dropout = 0.2))\n",
    "model_text.add(Dense(vocab_size+1, activation='softmax'))\n",
    "\n",
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 639971 samples, validate on 159993 samples\n",
      "Epoch 1/3\n",
      "639971/639971 [==============================] - 821s 1ms/sample - loss: 3.1178 - acc: 0.6248 - val_loss: 2.8764 - val_acc: 0.6243\n",
      "Epoch 2/3\n",
      "639971/639971 [==============================] - 460s 718us/sample - loss: 2.8093 - acc: 0.6264 - val_loss: 2.8129 - val_acc: 0.6264\n",
      "Epoch 3/3\n",
      "639971/639971 [==============================] - 451s 705us/sample - loss: 2.7592 - acc: 0.6280 - val_loss: 2.7900 - val_acc: 0.6285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c134acc50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks=[EarlyStopping(patience=1, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath='model_last.{epoch:02d}-{val_loss:.2f}.h5')]\n",
    "\n",
    "model_text.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_text.fit(X, y,\n",
    "          #callbacks=callbacks,\n",
    "          batch_size=32, epochs=3, verbose=1,\n",
    "          shuffle=True, validation_split=0.2,\n",
    "          workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_grams(X, n=4):\n",
    "    \"\"\"\n",
    "    This function takes a sequence of tupled sentences and tags:\n",
    "    [('Here is an example', 'tag1 tag2 tag3 tag4')]\n",
    "    Returns a tuple with other two tuples inside and the target (y).\n",
    "    The first tuple is for the encoded texts and the second for the encoded tags.\n",
    "    Each tuple contains two arrays:\n",
    "    1. The first array contains padded n-grams combinations(X).\n",
    "    2. The second array is composed of the next n words.\n",
    "    \n",
    "    All returned sequences are already pre-padded with 0.\n",
    "    The maximum length of the sequences is n-1, with n being the desired n-grams.\n",
    "    \"\"\"  \n",
    "    \n",
    "    text_sequences = list()\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        text_sent_sequences = list()\n",
    "        first_sent_words = list()\n",
    "        \n",
    "        for j in range(len(X[i])):\n",
    "            \n",
    "            first_words = X[i][:j+2]\n",
    "            text_sequence = X[i][j:j+n+1]\n",
    "            # avoinding repeated n-grams in the same sentence\n",
    "            if text_sequence not in text_sent_sequences and len(text_sequence)>1 and len(text_sequence)<=n:\n",
    "                text_sent_sequences += [text_sequence]\n",
    "                first_sent_words += [first_words]\n",
    "\n",
    "        if len(text_sent_sequences)>1:\n",
    "            text_sequences += first_sent_words + text_sent_sequences \n",
    "\n",
    "    print('Total Sequences: %d' % len(text_sequences))\n",
    "    X = pad_sequences([seq[:-1] for seq in text_sequences], maxlen=n)\n",
    "    y = np.array([[seq[-1]] for seq in text_sequences])\n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 799964\n",
      "Vocabulary Size: 85626\n",
      "Max length: 5\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/quotes_clean.pkl', 'rb') as file:\n",
    "    corpus = pickle.load(file)\n",
    "      \n",
    "text = [[['<bos>'] + words.split() + ['<eos>']][0] for words in corpus[:100000]]\n",
    "\n",
    "# integer encode sequences of words\n",
    "t_text = Tokenizer(filters='', oov_token=1)\n",
    "t_text.fit_on_texts(text)\n",
    "enc_texts = t_text.texts_to_sequences(text)\n",
    "\n",
    "seq2word_map = dict(map(reversed, t_text.word_index.items()))\n",
    "\n",
    "with open('../Trained_Weights/tokenizer_dict.json', 'w') as file:\n",
    "    json.dump(seq2word_map, file)\n",
    "    \n",
    "with open('../Trained_Weights/tokenizer_text.pkl', 'wb') as file:\n",
    "    pickle.dump(t_text, file)\n",
    "    \n",
    "#max_length = max([len(sent) for sent in enc_texts])\n",
    "vocab_size = len(set([word for words in enc_texts for word in words]))\n",
    "\n",
    "X, y = text_grams(enc_texts, n=5)\n",
    "\n",
    "max_length = X.shape[1]\n",
    "\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "print('Max length:', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove embedding\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(\"../Data/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "        \n",
    "embedding_matrix = np.zeros((len(t_text.word_index) + 1, 300))\n",
    "\n",
    "for word, i in t_text.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using n-grams of sentences, tags, reversed sentences and reversed tags\n",
    "# concatenating each model to get a more reliable prediction \n",
    "with tf.device('/cpu:0'):  \n",
    "    text_input = Input((max_length,))\n",
    "    model_text = Embedding(len(embedding_matrix), 50, weights=[embedding_matrix], trainable=False)(text_input)\n",
    "    model_text = TimeDistributed(Dense(len(embedding_matrix), activation = 'linear'))(model_text)\n",
    "\n",
    "model_text = Bidirectional(LSTM(32, return_sequences=True, dropout=0.2))(model_text)\n",
    "model_text = TimeDistributed(Dense(1000, activation='selu'))(model_text)\n",
    "model_text = Bidirectional(LSTM(8, return_sequences=True, dropout=0.2))(model_text)\n",
    "\n",
    "#tags_input = Input((max_length,))\n",
    "#model_tags = Embedding(tags_size, 32)(tags_input)\n",
    "#model_tags = Bidirectional(LSTM(8, dropout=0.2))(model_tags)\n",
    "#model_tags = TimeDistributed(Dense(1, activation='selu'))(model_tags)\n",
    "#model_tags = Bidirectional(LSTM(4, dropout=0.2))(model_tags)\n",
    "\n",
    "#concatenate = Concatenate()([model_text, model_tags])\n",
    "\"\"\"\n",
    "rev_text_input = Input((max_length,))\n",
    "rev_model_text = Embedding(embed_size, 64)(rev_text_input)\n",
    "rev_model_text = Bidirectional(LSTM(8, dropout=0.2, recurrent_dropout=0.1))(rev_model_text)\n",
    "\n",
    "rev_tags_input = Input((max_length,))\n",
    "rev_model_tags = Embedding(embed_size, 64)(rev_tags_input)\n",
    "rev_model_tags = Bidirectional(LSTM(8, dropout=0.2, recurrent_dropout=0.1))(rev_model_tags)\n",
    "\n",
    "rev_concatenate_text_tags = Concatenate()([rev_model_text, rev_model_tags])\n",
    "\n",
    "concatenate = Concatenate()([concatenate_text_tags, rev_concatenate_text_tags])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#model = TimeDistributed(Dense(vocab_size+2, activation='selu'))(concatenate)\n",
    "#model = Bidirectional(LSTM(4, dropout=0.2))(model)\n",
    "#rpt = RepeatVector(1)(concatenate)\n",
    "\n",
    "result = TimeDistributed(Dense(vocab_size+2, activation='softmax'))(model_text)\n",
    "\n",
    "model = Model(inputs=text_input, outputs=result)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[EarlyStopping(patience=1, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath='model_last.{epoch:02d}-{val_loss:.2f}.h5')]\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y,\n",
    "          callbacks=callbacks,\n",
    "          batch_size=32, epochs=5, verbose=1,\n",
    "          #shuffle=True, validation_split=0.2,\n",
    "          workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('quotes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:1], y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
